{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人脸检测和识别推理流程\n",
    "\n",
    "以下示例展示了如何使用facenet_pytorch python包，在使用在VGGFace2数据集上预训练的Inception Resnet V1模型上对图像数据集执行人脸检测和识别。\n",
    "\n",
    "以下PyTorch方法已包含：\n",
    "\n",
    "* 数据集\n",
    "* 数据加载器\n",
    "* GPU/CPU处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T11:23:56.432302Z",
     "start_time": "2023-07-20T11:23:54.333487Z"
    }
   },
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "workers = 0 if os.name == 'nt' else 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 判断是否有nvidia GPU可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T11:23:56.470217Z",
     "start_time": "2023-07-20T11:23:56.436290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在该设备上运行: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('在该设备上运行: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义MTCNN模块\n",
    "\n",
    "为了说明，默认参数已显示，但不是必需的。请注意，由于MTCNN是一组神经网络和其他代码，因此必须以以下方式传递设备，以便在需要内部复制对象时启用。\n",
    "\n",
    "查看`help(MTCNN)`获取更多细节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T11:23:56.587926Z",
     "start_time": "2023-07-20T11:23:56.472212Z"
    }
   },
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义Inception Resnet V1模块\n",
    "\n",
    "设置classify=True以使用预训练分类器。对于本示例，我们将使用该模型输出嵌入/卷积特征。请注意，在推理过程中，将模型设置为`eval`模式非常重要。\n",
    "\n",
    "查看`help(InceptionResnetV1)`获取更多细节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T11:23:56.988662Z",
     "start_time": "2023-07-20T11:23:56.588910Z"
    }
   },
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义数据集和数据加载器\n",
    "\n",
    "我们向数据集添加了`idx_to_class`属性，以便稍后轻松重编标签索引为身份名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T11:23:56.995647Z",
     "start_time": "2023-07-20T11:23:56.989657Z"
    }
   },
   "outputs": [],
   "source": [
    "# 返回批次的第一个样本\n",
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "dataset = datasets.ImageFolder('./data/test_images') ##. 表示同级目录\n",
    "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()} # 创建字典表示id 和类名的对应关系\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn, num_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 执行MTCNN人脸检测\n",
    "\n",
    "迭代DataLoader对象并检测每个人脸及其关联的检测概率。如果检测到脸部，`MTCNN`的前向方法将返回裁剪到检测到的脸部的图像。默认情况下，仅返回检测到的单个面部-要使`MTCNN`返回所有检测到的面部，请在上面创建MTCNN对象时设置`keep_all=True`。\n",
    "\n",
    "要获取边界框而不是裁剪的人脸图像，可以调用较低级别的`mtcnn.detect()`函数。查看`help(mtcnn.detect)`获取详细信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T11:24:00.971832Z",
     "start_time": "2023-07-20T11:23:56.996640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=1665x2048 at 0x2716FEB4320> 0\n",
      "检测到的人脸及其概率: 0.999983\n",
      "<PIL.Image.Image image mode=RGB size=2341x3600 at 0x271647C8C20> 1\n",
      "检测到的人脸及其概率: 0.999934\n",
      "<PIL.Image.Image image mode=RGB size=1125x1612 at 0x2716FF65430> 2\n",
      "检测到的人脸及其概率: 0.999733\n",
      "<PIL.Image.Image image mode=RGB size=1638x2048 at 0x2716FB74140> 3\n",
      "检测到的人脸及其概率: 0.999880\n",
      "<PIL.Image.Image image mode=RGB size=2048x1991 at 0x2716FF65490> 4\n",
      "检测到的人脸及其概率: 0.999992\n",
      "<PIL.Image.Image image mode=RGB size=2102x3000 at 0x2716FEB5FD0> 5\n",
      "检测到的人脸及其概率: 0.999597\n",
      "<PIL.Image.Image image mode=RGB size=678x1206 at 0x2716FC23E90> 6\n",
      "检测到的人脸及其概率: 1.000000\n"
     ]
    }
   ],
   "source": [
    "aligned = [] # 存储检测到的人脸\n",
    "names = [] \n",
    "# x: 图片数据\n",
    "# y: 图片对应的标签\n",
    "for x, y in loader:\n",
    "    print(x,y)\n",
    "    x_aligned, prob = mtcnn(x, return_prob=True)\n",
    "    if x_aligned is not None:\n",
    "        print('检测到的人脸及其概率: {:8f}'.format(prob))\n",
    "        aligned.append(x_aligned)\n",
    "        names.append(dataset.idx_to_class[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# # 遍历 aligned 列表中的图片\n",
    "# for i, face in enumerate(aligned):\n",
    "#     # 转换为 NumPy 数组并转换为 PIL 图像\n",
    "#     face_image = Image.fromarray((face.permute(1, 2, 0).cpu().numpy() * 255).astype('uint8'))\n",
    "#     face_image.show(title=f\"Aligned Face {i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算图像嵌入\n",
    "\n",
    "MTCNN将返回所有面部图像的相同大小，从而可以使用Resnet识别模块轻松进行批处理。在这里，由于我们只有一些图像，因此我们构建一个单个批次并对其执行推理。\n",
    "\n",
    "对于实际数据集，代码应修改为控制传递给Resnet的批处理大小，特别是如果在GPU上处理。对于重复测试，最好将人脸检测（使用MTCNN）与嵌入或分类（使用InceptionResnetV1）分开，因为剪切面或边界框的计算可以一次执行，检测到的面部可以保存供将来使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T11:24:01.037605Z",
     "start_time": "2023-07-20T11:24:00.973820Z"
    }
   },
   "outputs": [],
   "source": [
    "aligned = torch.stack(aligned).to(device)\n",
    "embeddings = resnet(aligned).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 0 belongs to cluster 5\n",
      "Embedding 1 belongs to cluster 2\n",
      "Embedding 2 belongs to cluster 0\n",
      "Embedding 3 belongs to cluster 4\n",
      "Embedding 4 belongs to cluster 3\n",
      "Embedding 5 belongs to cluster 1\n",
      "Embedding 6 belongs to cluster 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\anaconda\\V_environment\\Pytorch\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# 假设 embeddings 是一个 PyTorch 张量，先转换为 NumPy 数组\n",
    "embeddings_np = embeddings.numpy()\n",
    "\n",
    "# 设置聚类的类别数量（例如 3 类）\n",
    "num_clusters = 6\n",
    "\n",
    "# 使用 K-Means 聚类\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(embeddings_np)\n",
    "\n",
    "# 获取每个嵌入向量的聚类标签\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# 打印聚类结果\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"Embedding {i} belongs to cluster {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 打印各类别的距离矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T11:24:01.050571Z",
     "start_time": "2023-07-20T11:24:01.038602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the picture is yhj\n"
     ]
    }
   ],
   "source": [
    "# dists = [[(e1 - e2).norm().item() for e2 in embeddings] for e1 in embeddings]\n",
    "# pd.DataFrame(dists, columns=names, index=names)\n",
    "\n",
    "# 获取最后一个嵌入向量\n",
    "last_embedding = embeddings[-1]\n",
    "\n",
    "# 计算最后一个嵌入向量与前几个嵌入向量的欧几里得距离\n",
    "dists = [(last_embedding - e).norm().item() for e in embeddings[:-1]]\n",
    "\n",
    "# 打印结果\n",
    "index = np.argmin(dists)   \n",
    "print(\"the picture is \" + names[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image, ImageDraw\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = 'D:/courses/software/team-project-25spring-g10/AI/human_face/data/multiface.jpg'\n",
    "# image = Image.open(image_path)\n",
    "# image = image.convert('RGB')\n",
    "\n",
    "# # 检测人脸\n",
    "# boxes, _ = mtcnn.detect(image)\n",
    "\n",
    "# # 绘制人脸框\n",
    "# image_draw = image.copy()\n",
    "# draw = ImageDraw.Draw(image_draw)\n",
    "# if boxes is not None:\n",
    "#     for box in boxes:\n",
    "#         draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
